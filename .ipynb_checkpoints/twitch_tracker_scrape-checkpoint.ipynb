{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c6f8657",
   "metadata": {},
   "source": [
    "# <center>Scraping twitch tracker for the top 500 streamers for the top 10 games</center>\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f708c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bs4\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "pd.options.display.float_format = '{:.0f}'.format\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "path_to_driver = '/usr/bin/chromedriver'\n",
    "chrome = webdriver.Chrome(path_to_driver, options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15b3f4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_as_csv(df):\n",
    "    import os\n",
    "    path = r'./data/top_streamers.csv'\n",
    "    \n",
    "    if not os.path.exists(r'./data'):\n",
    "        os.mkdir(r'./data')\n",
    "    \n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "        df.to_csv(path, index=False)\n",
    "        return 'file overriden'\n",
    "    else:\n",
    "        df.to_csv(path, index=False)\n",
    "        return 'saved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1805f213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(path):\n",
    "    import os\n",
    "    if not os.path.isfile(path):\n",
    "        raise Exception('path does not exist')\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3b329f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_most_subscribed_streamers(driver=None, n=10):\n",
    "    if driver is None:\n",
    "        raise Exception('You have to provide a web driver')\n",
    "        \n",
    "    headers = ['#', 'streamer_name']\n",
    "    result = []\n",
    "    url = 'https://twitchtracker.com/subscribers?page='\n",
    "    for i in range(n):\n",
    "        print('extracting page ' + str(i+1))\n",
    "        driver.get(url + str(i+1))\n",
    "        html = driver.execute_script('return document.body.innerHTML;')\n",
    "        soup = bs(html, 'lxml')\n",
    "        for j in range(21):\n",
    "            if j+1 == 11:\n",
    "                continue\n",
    "            temp = []\n",
    "            streamer_position = soup.table.find_all('tr')[j+1].find_all('td')[0].text.replace('#', '')\n",
    "            streamer_name = soup.table.find_all('tr')[j+1].find_all('td')[3].a.text\n",
    "            temp.append([streamer_position, streamer_name])\n",
    "            result.append(temp)\n",
    "        time.sleep(3)\n",
    "    result = np.array(result)\n",
    "    result = result.squeeze()\n",
    "    return pd.DataFrame(result, columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3f5fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_streamer_monthly_info(driver=None, names=''):\n",
    "    if driver is None:\n",
    "        raise Exception('You have to provide a web driver')\n",
    "        \n",
    "    headers = ['streamer_name', 'hours_streamed', 'average_viewers', \n",
    "               'peak_viewers', 'hours_watched', 'followers_gained', \n",
    "               'followers_per_hour', 'viewers_gained', 'active_days']\n",
    "    \n",
    "    url = 'https://twitchtracker.com/'\n",
    "    result = []\n",
    "    \n",
    "    for name in names:\n",
    "        print('extracting: ' + name)\n",
    "        try:\n",
    "            driver.get(url + name)\n",
    "            month_button = driver.find_element_by_xpath('//*[@id=\"select-performance\"]/span[2]')\n",
    "            month_button.click()\n",
    "        except:\n",
    "            continue\n",
    "        html = driver.execute_script('return document.body.innerHTML;')\n",
    "        soup = bs(html, 'lxml')\n",
    "        \n",
    "        result_temp = []\n",
    "        result_temp.append([name])\n",
    "        for i in range(len(headers)):\n",
    "            temp = []\n",
    "            if i < 7:\n",
    "                data = soup.find_all('div', {'id': 'performance-panel'})[0].find_all('div', {'class': 'g-x-s-value g-x-s-contrast'})[i].find_all('span')[0].text\n",
    "                temp.append(data)\n",
    "            elif i == 7:\n",
    "                active_days = soup.find_all('div', {'id': 'performance-panel'})[0].find_all('div', {'class': 'g-x-s-value g-x-s-contrast'})[i].find_all('span')[0].text.split('/')[0]\n",
    "                temp.append(active_days)\n",
    "            if len(temp) >= 1:\n",
    "                result_temp.append(temp)\n",
    "        result.append(result_temp)\n",
    "        time.sleep(3)\n",
    "    df = pd.DataFrame(np.array(result).reshape(len(result), 9), columns=headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744260a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = r'./data/top_streamers.csv'\n",
    "#df = get_top_n_most_subscribed_streamers(driver=chrome, n=50)\n",
    "#save_df_as_csv(df)\n",
    "df = load_csv(path)\n",
    "streamer_names = np.array(df['streamer_name'])\n",
    "hui = get_streamer_monthly_info(chrome, streamer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "900eac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "hui.to_csv('./data/hui.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb25f281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99ef9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_streamer_weekly_info(driver=None, names=''):\n",
    "    if driver is None:\n",
    "        raise Exception('You have to provide a web driver')\n",
    "        \n",
    "    headers = ['streamer_name', 'hours_streamed', 'average_viewers', \n",
    "               'peak_viewers', 'hours_watched', 'followers_gained', \n",
    "               'followers_per_hour', 'viewers_gained', 'active_days']\n",
    "    \n",
    "    url = 'https://twitchtracker.com/'\n",
    "    result = []\n",
    "    \n",
    "    for name in names:\n",
    "        print('extracting: ' + name)\n",
    "        try:\n",
    "            driver.get(url + name)\n",
    "            week_button = driver.find_element_by_xpath('//*[@id=\"select-performance\"]/span[1]')\n",
    "            week_button.click()\n",
    "        except:\n",
    "            continue\n",
    "        html = driver.execute_script('return document.body.innerHTML;')\n",
    "        soup = bs(html, 'lxml')\n",
    "        \n",
    "        result_temp = []\n",
    "        result_temp.append([name])\n",
    "        for i in range(len(headers)):\n",
    "            try:\n",
    "                temp = []\n",
    "                if i < 7:\n",
    "                    data = soup.find_all('div', {'id': 'performance-panel'})[0].find_all('div', {'class': 'g-x-s-value g-x-s-contrast'})[i].find_all('span')[0].text\n",
    "                    temp.append(data)\n",
    "                elif i == 7:\n",
    "                    active_days = soup.find_all('div', {'id': 'performance-panel'})[0].find_all('div', {'class': 'g-x-s-value g-x-s-contrast'})[i].find_all('span')[0].text.split('/')[0]\n",
    "                    temp.append(active_days)\n",
    "                if len(temp) >= 1:\n",
    "                    result_temp.append(temp)\n",
    "            except:\n",
    "                continue\n",
    "        result.append(result_temp)\n",
    "        time.sleep(3)\n",
    "    df = pd.DataFrame(np.array(result).reshape(len(result), 9), columns=headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a55664",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kur = get_streamer_weekly_info(chrome, streamer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4845ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kur.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1118c6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kur.to_csv('./data/weekly_kur.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc16526",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
